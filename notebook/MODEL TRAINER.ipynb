{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b667ba7",
   "metadata": {},
   "source": [
    "--------\n",
    "# Try - Model Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c459715e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from xgboost import XGBClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d04c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../artifacts/train_set.csv\")\n",
    "test_df = pd.read_csv(\"../artifacts/test_set.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf89959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop features 1712 to 1734\n",
    "features_to_drop = [f'Feature_{i}' for i in range(1712, 1735)]\n",
    "train_df = train_df.drop(columns=features_to_drop)\n",
    "test_df = test_df.drop(columns=features_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5663cf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce Multicollinearity (for both datasets)\n",
    "# Drop Feature_2, Feature_6, Feature_7, Feature_8, Feature_9, Feature_2032\n",
    "cols_to_drop = ['Feature_2', 'Feature_6', 'Feature_7', 'Feature_8', 'Feature_9', 'Feature_2032']\n",
    "train_df = train_df.drop(columns=cols_to_drop)\n",
    "test_df = test_df.drop(columns=cols_to_drop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a0c586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clip Extreme Values Across All Features (5th to 95th percentiles)\n",
    "numeric_cols = train_df.drop(columns=['ID', 'CLASS']).columns\n",
    "for col in numeric_cols:\n",
    "    # Compute bounds on training data\n",
    "    lower_bound, upper_bound = train_df[col].quantile([0.05, 0.95])\n",
    "    # Apply clipping to both train and test\n",
    "    train_df[col] = train_df[col].clip(lower=lower_bound, upper=upper_bound)\n",
    "    test_df[col] = test_df[col].clip(lower=lower_bound, upper=upper_bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f68ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Features and Target\n",
    "X_train = train_df.drop(columns=['ID', 'CLASS'])\n",
    "y_train = train_df['CLASS']\n",
    "X_test = test_df.drop(columns=['ID', 'CLASS'])\n",
    "y_test = test_df['CLASS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee20fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Near-Constant Features\n",
    "stds = X_train.std()\n",
    "low_variance_cols = stds[stds < 1e-6].index\n",
    "print(f\"Dropping {len(low_variance_cols)} near-constant features: {low_variance_cols}\")\n",
    "X_train = X_train.drop(columns=low_variance_cols)\n",
    "X_test = X_test.drop(columns=low_variance_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9c396d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling (fit on training data, transform both)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7338149e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping 159 near-constant features: Index(['Feature_1910', 'Feature_1911', 'Feature_1912', 'Feature_1913',\n",
      "       'Feature_1914', 'Feature_1915', 'Feature_1916', 'Feature_1917',\n",
      "       'Feature_1918', 'Feature_1919',\n",
      "       ...\n",
      "       'Feature_2981', 'Feature_2982', 'Feature_3097', 'Feature_3098',\n",
      "       'Feature_3104', 'Feature_3107', 'Feature_3225', 'Feature_3226',\n",
      "       'Feature_3232', 'Feature_3235'],\n",
      "      dtype='object', length=159)\n",
      "Top 10 Feature Importances:\n",
      "Feature_3038    0.005962\n",
      "Feature_1982    0.004320\n",
      "Feature_1905    0.003341\n",
      "Feature_3085    0.002977\n",
      "Feature_3166    0.002975\n",
      "Feature_2965    0.002781\n",
      "Feature_3150    0.002723\n",
      "Feature_1701    0.002636\n",
      "Feature_2909    0.002607\n",
      "Feature_2912    0.002572\n",
      "dtype: float64\n",
      "Best Random Forest Parameters: {'max_depth': None, 'min_samples_split': 5, 'n_estimators': 200}\n",
      "Random Forest Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.78      0.70        58\n",
      "           1       0.57      0.40      0.47        42\n",
      "\n",
      "    accuracy                           0.62       100\n",
      "   macro avg       0.60      0.59      0.59       100\n",
      "weighted avg       0.61      0.62      0.61       100\n",
      "\n",
      "Best XGBoost Parameters: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 300}\n",
      "Tuned XGBoost Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.79      0.70        58\n",
      "           1       0.54      0.33      0.41        42\n",
      "\n",
      "    accuracy                           0.60       100\n",
      "   macro avg       0.58      0.56      0.55       100\n",
      "weighted avg       0.59      0.60      0.58       100\n",
      "\n",
      "Best Logistic Regression Parameters: {'C': 0.01, 'solver': 'liblinear'}\n",
      "Tuned Logistic Regression Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.66      0.68        58\n",
      "           1       0.57      0.62      0.59        42\n",
      "\n",
      "    accuracy                           0.64       100\n",
      "   macro avg       0.63      0.64      0.63       100\n",
      "weighted avg       0.65      0.64      0.64       100\n",
      "\n",
      "Random Forest Cross-Validation F1-Scores (on training data): [0.60869565 0.46511628 0.47368421 0.43137255 0.5       ]\n",
      "Average CV F1-Score for Random Forest: 0.496 (+/- 0.121)\n",
      "XGBoost Cross-Validation F1-Scores (on training data): [0.47826087 0.3902439  0.5        0.49056604 0.55319149]\n",
      "Average CV F1-Score for XGBoost: 0.482 (+/- 0.105)\n",
      "Logistic Regression Cross-Validation F1-Scores (on training data): [0.55555556 0.54901961 0.55319149 0.53333333 0.58823529]\n",
      "Average CV F1-Score for Logistic Regression: 0.556 (+/- 0.036)\n"
     ]
    }
   ],
   "source": [
    "# Check for infinite/NaN values after scaling\n",
    "for X, name in [(X_train_scaled, 'train'), (X_test_scaled, 'test')]:\n",
    "    if np.any(np.isinf(X)) or np.any(np.isnan(X)):\n",
    "        print(f\"Infinite or NaN values found in {name} set after scaling. Replacing with 0...\")\n",
    "        X = X.replace([np.inf, -np.inf], 0)\n",
    "        X = X.fillna(0)\n",
    "\n",
    "# Step 9: Feature Selection using Random Forest Importance (based on training data)\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Select top 150 features\n",
    "feature_importance = pd.Series(rf.feature_importances_, index=X_train_scaled.columns)\n",
    "top_features = feature_importance.nlargest(150).index\n",
    "X_train_selected = X_train_scaled[top_features]\n",
    "X_test_selected = X_test_scaled[top_features]\n",
    "print(\"Top 10 Feature Importances:\")\n",
    "print(feature_importance.nlargest(10))\n",
    "\n",
    "# Step 10: Random Forest (Enhanced Tuning)\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [200, 300],\n",
    "    'max_depth': [None, 20],\n",
    "    'min_samples_split': [2, 5]\n",
    "}\n",
    "rf_grid = GridSearchCV(RandomForestClassifier(random_state=42, class_weight='balanced'), rf_param_grid, cv=5, scoring='f1', n_jobs=-1)\n",
    "rf_grid.fit(X_train_selected, y_train)\n",
    "rf_best = rf_grid.best_estimator_\n",
    "y_pred_rf = rf_best.predict(X_test_selected)\n",
    "print(f\"Best Random Forest Parameters: {rf_grid.best_params_}\")\n",
    "print(\"Random Forest Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "\n",
    "# Step 11: Tune XGBoost\n",
    "xgb_param_grid = {\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [5, 7, 10],\n",
    "    'n_estimators': [200, 300]\n",
    "}\n",
    "xgb_grid = GridSearchCV(XGBClassifier(random_state=42, eval_metric='logloss'), xgb_param_grid, cv=5, scoring='f1', n_jobs=-1)\n",
    "xgb_grid.fit(X_train_selected, y_train)\n",
    "xgb_best = xgb_grid.best_estimator_\n",
    "y_pred_xgb = xgb_best.predict(X_test_selected)\n",
    "print(f\"Best XGBoost Parameters: {xgb_grid.best_params_}\")\n",
    "print(\"Tuned XGBoost Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_xgb))\n",
    "\n",
    "# Step 12: Tune Logistic Regression\n",
    "lr_param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10],\n",
    "    'solver': ['liblinear', 'lbfgs']\n",
    "}\n",
    "lr_grid = GridSearchCV(LogisticRegression(class_weight='balanced', random_state=42, max_iter=1000), lr_param_grid, cv=5, scoring='f1', n_jobs=-1)\n",
    "lr_grid.fit(X_train_selected, y_train)\n",
    "lr_best = lr_grid.best_estimator_\n",
    "y_pred_lr = lr_best.predict(X_test_selected)\n",
    "print(f\"Best Logistic Regression Parameters: {lr_grid.best_params_}\")\n",
    "print(\"Tuned Logistic Regression Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_lr))\n",
    "\n",
    "# Step 13: Cross-Validation F1-Score for All Models (on training data)\n",
    "models = {\n",
    "    'Random Forest': rf_best,\n",
    "    'XGBoost': xgb_best,\n",
    "    'Logistic Regression': lr_best\n",
    "}\n",
    "for name, model in models.items():\n",
    "    cv_scores = cross_val_score(model, X_train_selected, y_train, cv=5, scoring='f1')\n",
    "    print(f\"{name} Cross-Validation F1-Scores (on training data): {cv_scores}\")\n",
    "    print(f\"Average CV F1-Score for {name}: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f013f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../artifacts/train_set.csv\")\n",
    "test_df = pd.read_csv(\"../artifacts/test_set.csv\")\n",
    "\n",
    "# Load the unseen dataset (without CLASS column)\n",
    "# Adjust file path as needed\n",
    "unseen_df = pd.read_csv(\"../artifacts/blinded_test_set.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7473bdbf",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9c67eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping 159 near-constant features: Index(['Feature_1910', 'Feature_1911', 'Feature_1912', 'Feature_1913',\n",
      "       'Feature_1914', 'Feature_1915', 'Feature_1916', 'Feature_1917',\n",
      "       'Feature_1918', 'Feature_1919',\n",
      "       ...\n",
      "       'Feature_2981', 'Feature_2982', 'Feature_3097', 'Feature_3098',\n",
      "       'Feature_3104', 'Feature_3107', 'Feature_3225', 'Feature_3226',\n",
      "       'Feature_3232', 'Feature_3235'],\n",
      "      dtype='object', length=159)\n",
      "Top 10 Feature Importances:\n",
      "Feature_3038    0.005962\n",
      "Feature_1982    0.004320\n",
      "Feature_1905    0.003341\n",
      "Feature_3085    0.002977\n",
      "Feature_3166    0.002975\n",
      "Feature_2965    0.002781\n",
      "Feature_3150    0.002723\n",
      "Feature_1701    0.002636\n",
      "Feature_2909    0.002607\n",
      "Feature_2912    0.002572\n",
      "dtype: float64\n",
      "\n",
      "Random Forest Metrics (on Test Set):\n",
      "Accuracy: 0.620\n",
      "AUROC: 0.659\n",
      "Sensitivity (Recall/TPR): 0.405\n",
      "Specificity (TNR): 0.776\n",
      "F1-score (class 1): 0.472\n",
      "F1-score (macro avg): 0.588\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.78      0.70        58\n",
      "           1       0.57      0.40      0.47        42\n",
      "\n",
      "    accuracy                           0.62       100\n",
      "   macro avg       0.60      0.59      0.59       100\n",
      "weighted avg       0.61      0.62      0.61       100\n",
      "\n",
      "Best Random Forest Parameters: {'max_depth': None, 'min_samples_split': 5, 'n_estimators': 200}\n",
      "\n",
      "XGBoost Metrics (on Test Set):\n",
      "Accuracy: 0.600\n",
      "AUROC: 0.649\n",
      "Sensitivity (Recall/TPR): 0.333\n",
      "Specificity (TNR): 0.793\n",
      "F1-score (class 1): 0.412\n",
      "F1-score (macro avg): 0.554\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.79      0.70        58\n",
      "           1       0.54      0.33      0.41        42\n",
      "\n",
      "    accuracy                           0.60       100\n",
      "   macro avg       0.58      0.56      0.55       100\n",
      "weighted avg       0.59      0.60      0.58       100\n",
      "\n",
      "Best XGBoost Parameters: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 300}\n",
      "\n",
      "Logistic Regression Metrics (on Test Set):\n",
      "Accuracy: 0.610\n",
      "AUROC: 0.622\n",
      "Sensitivity (Recall/TPR): 0.524\n",
      "Specificity (TNR): 0.672\n",
      "F1-score (class 1): 0.530\n",
      "F1-score (macro avg): 0.598\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.67      0.67        58\n",
      "           1       0.54      0.52      0.53        42\n",
      "\n",
      "    accuracy                           0.61       100\n",
      "   macro avg       0.60      0.60      0.60       100\n",
      "weighted avg       0.61      0.61      0.61       100\n",
      "\n",
      "Best Logistic Regression Parameters: {'C': 10, 'solver': 'lbfgs'}\n",
      "\n",
      "Best Model: Logistic Regression with Macro Avg F1-score: 0.598\n",
      "Best model saved as 'logistic_regression_model.pkl'\n",
      "\n",
      "Random Forest Cross-Validation F1-Scores (on training data): [0.60869565 0.46511628 0.47368421 0.43137255 0.5       ]\n",
      "Average CV F1-Score for Random Forest: 0.496 (+/- 0.121)\n",
      "\n",
      "XGBoost Cross-Validation F1-Scores (on training data): [0.47826087 0.3902439  0.5        0.49056604 0.55319149]\n",
      "Average CV F1-Score for XGBoost: 0.482 (+/- 0.105)\n",
      "\n",
      "Logistic Regression Cross-Validation F1-Scores (on training data): [0.54166667 0.55555556 0.57142857 0.58823529 0.52      ]\n",
      "Average CV F1-Score for Logistic Regression: 0.555 (+/- 0.047)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, roc_auc_score, confusion_matrix\n",
    "from xgboost import XGBClassifier\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "train_df = pd.read_csv(\"../artifacts/train_set.csv\")\n",
    "test_df = pd.read_csv(\"../artifacts/test_set.csv\")\n",
    "\n",
    "# Load the unseen dataset (without CLASS column)\n",
    "unseen_df = pd.read_csv(\"../artifacts/blinded_test_set.csv\")\n",
    "\n",
    "# Step 2: Handle Missing Values (for all datasets)\n",
    "# Drop features 1712 to 1734\n",
    "features_to_drop = [f'Feature_{i}' for i in range(1712, 1735)]\n",
    "train_df = train_df.drop(columns=features_to_drop)\n",
    "test_df = test_df.drop(columns=features_to_drop)\n",
    "\n",
    "# Step 3: Reduce Multicollinearity (for all datasets)\n",
    "# Drop Feature_2, Feature_6, Feature_7, Feature_8, Feature_9, Feature_2032\n",
    "cols_to_drop = ['Feature_2', 'Feature_6', 'Feature_7', 'Feature_8', 'Feature_9', 'Feature_2032']\n",
    "train_df = train_df.drop(columns=cols_to_drop)\n",
    "test_df = test_df.drop(columns=cols_to_drop)\n",
    "\n",
    "# Step 4: Clip Extreme Values Across All Features (5th to 95th percentiles)\n",
    "numeric_cols = train_df.drop(columns=['ID', 'CLASS']).columns\n",
    "for col in numeric_cols:\n",
    "    # Compute bounds on training data\n",
    "    lower_bound, upper_bound = train_df[col].quantile([0.05, 0.95])\n",
    "    # Apply clipping to all datasets\n",
    "    train_df[col] = train_df[col].clip(lower=lower_bound, upper=upper_bound)\n",
    "    test_df[col] = test_df[col].clip(lower=lower_bound, upper=upper_bound)\n",
    "\n",
    "# Step 6: Prepare Features and Target (Skipping Log Transformation)\n",
    "X_train = train_df.drop(columns=['ID', 'CLASS'])\n",
    "y_train = train_df['CLASS']\n",
    "X_test = test_df.drop(columns=['ID', 'CLASS'])\n",
    "y_test = test_df['CLASS']\n",
    "\n",
    "# Step 7: Drop Near-Constant Features (based on training data)\n",
    "stds = X_train.std()\n",
    "low_variance_cols = stds[stds < 1e-6].index\n",
    "print(f\"Dropping {len(low_variance_cols)} near-constant features: {low_variance_cols}\")\n",
    "X_train = X_train.drop(columns=low_variance_cols)\n",
    "X_test = X_test.drop(columns=low_variance_cols)\n",
    "\n",
    "# Step 8: Feature Scaling (fit on training data, transform all)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
    "\n",
    "# Check for infinite/NaN values after scaling\n",
    "for X, name in [(X_train_scaled, 'train'), (X_test_scaled, 'test')]:\n",
    "    if np.any(np.isinf(X)) or np.any(np.isnan(X)):\n",
    "        print(f\"Infinite or NaN values found in {name} set after scaling. Replacing with 0...\")\n",
    "        X = X.replace([np.inf, -np.inf], 0)\n",
    "        X = X.fillna(0)\n",
    "\n",
    "# Step 9: Feature Selection using Random Forest Importance (based on training data)\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Select top 150 features\n",
    "feature_importance = pd.Series(rf.feature_importances_, index=X_train_scaled.columns)\n",
    "top_features = feature_importance.nlargest(150).index\n",
    "X_train_selected = X_train_scaled[top_features]\n",
    "X_test_selected = X_test_scaled[top_features]\n",
    "print(\"Top 10 Feature Importances:\")\n",
    "print(feature_importance.nlargest(10))\n",
    "\n",
    "# Function to compute all metrics\n",
    "def compute_metrics(y_true, y_pred, y_pred_proba, model_name):\n",
    "    print(f\"\\n{model_name} Metrics (on Test Set):\")\n",
    "    # Accuracy\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    print(f\"Accuracy: {accuracy:.3f}\")\n",
    "    \n",
    "    # AUROC\n",
    "    auroc = roc_auc_score(y_true, y_pred_proba)\n",
    "    print(f\"AUROC: {auroc:.3f}\")\n",
    "    \n",
    "    # Confusion Matrix for Sensitivity and Specificity\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    sensitivity = tp / (tp + fn)  # Recall/TPR for class 1\n",
    "    specificity = tn / (tn + fp)  # TNR for class 0\n",
    "    print(f\"Sensitivity (Recall/TPR): {sensitivity:.3f}\")\n",
    "    print(f\"Specificity (TNR): {specificity:.3f}\")\n",
    "    \n",
    "    # F1-score for class 1\n",
    "    f1_class1 = f1_score(y_true, y_pred, pos_label=1)\n",
    "    print(f\"F1-score (class 1): {f1_class1:.3f}\")\n",
    "    \n",
    "    # Macro average F1-score\n",
    "    f1_macro = f1_score(y_true, y_pred, average='macro')\n",
    "    print(f\"F1-score (macro avg): {f1_macro:.3f}\")\n",
    "    \n",
    "    # Classification Report\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    \n",
    "    return f1_macro\n",
    "\n",
    "# Step 10: Random Forest (Enhanced Tuning)\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [200, 300],\n",
    "    'max_depth': [None, 20],\n",
    "    'min_samples_split': [2, 5]\n",
    "}\n",
    "rf_grid = GridSearchCV(RandomForestClassifier(random_state=42, class_weight='balanced'), rf_param_grid, cv=5, scoring='f1', n_jobs=-1)\n",
    "rf_grid.fit(X_train_selected, y_train)\n",
    "rf_best = rf_grid.best_estimator_\n",
    "y_pred_rf = rf_best.predict(X_test_selected)\n",
    "y_pred_rf_proba = rf_best.predict_proba(X_test_selected)[:, 1]  # Probabilities for class 1\n",
    "f1_rf = compute_metrics(y_test, y_pred_rf, y_pred_rf_proba, \"Random Forest\")\n",
    "print(f\"Best Random Forest Parameters: {rf_grid.best_params_}\")\n",
    "\n",
    "# Step 11: Tune XGBoost\n",
    "xgb_param_grid = {\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [5, 7, 10],\n",
    "    'n_estimators': [200, 300]\n",
    "}\n",
    "xgb_grid = GridSearchCV(XGBClassifier(random_state=42, eval_metric='logloss'), xgb_param_grid, cv=5, scoring='f1', n_jobs=-1)\n",
    "xgb_grid.fit(X_train_selected, y_train)\n",
    "xgb_best = xgb_grid.best_estimator_\n",
    "y_pred_xgb = xgb_best.predict(X_test_selected)\n",
    "y_pred_xgb_proba = xgb_best.predict_proba(X_test_selected)[:, 1]\n",
    "f1_xgb = compute_metrics(y_test, y_pred_xgb, y_pred_xgb_proba, \"XGBoost\")\n",
    "print(f\"Best XGBoost Parameters: {xgb_grid.best_params_}\")\n",
    "\n",
    "# Step 12: Tune Logistic Regression\n",
    "lr_param_grid = {\n",
    "    'C': [0.5, 1, 2, 5, 10],\n",
    "    'solver': ['liblinear', 'lbfgs']\n",
    "}\n",
    "lr_grid = GridSearchCV(LogisticRegression(class_weight={0: 1, 1: 1.5}, random_state=42, max_iter=1000), lr_param_grid, cv=5, scoring='f1', n_jobs=-1)\n",
    "lr_grid.fit(X_train_selected, y_train)\n",
    "lr_best = lr_grid.best_estimator_\n",
    "y_pred_lr = lr_best.predict(X_test_selected)\n",
    "y_pred_lr_proba = lr_best.predict_proba(X_test_selected)[:, 1]\n",
    "f1_lr = compute_metrics(y_test, y_pred_lr, y_pred_lr_proba, \"Logistic Regression\")\n",
    "print(f\"Best Logistic Regression Parameters: {lr_grid.best_params_}\")\n",
    "\n",
    "# Step 13: Select and Save the Best Model\n",
    "# Compare Macro Average F1-score to select the best model\n",
    "f1_scores = {\n",
    "    'Random Forest': f1_rf,\n",
    "    'XGBoost': f1_xgb,\n",
    "    'Logistic Regression': f1_lr\n",
    "}\n",
    "best_model_name = max(f1_scores, key=f1_scores.get)\n",
    "best_model = rf_best if best_model_name == 'Random Forest' else xgb_best if best_model_name == 'XGBoost' else lr_best\n",
    "print(f\"\\nBest Model: {best_model_name} with Macro Avg F1-score: {f1_scores[best_model_name]:.3f}\")\n",
    "joblib.dump(best_model, f'{best_model_name.lower().replace(\" \", \"_\")}_model.pkl')\n",
    "print(f\"Best model saved as '{best_model_name.lower().replace(' ', '_')}_model.pkl'\")\n",
    "\n",
    "# Step 14: Cross-Validation F1-Score for All Models (on training data)\n",
    "models = {\n",
    "    'Random Forest': rf_best,\n",
    "    'XGBoost': xgb_best,\n",
    "    'Logistic Regression': lr_best\n",
    "}\n",
    "for name, model in models.items():\n",
    "    cv_scores = cross_val_score(model, X_train_selected, y_train, cv=5, scoring='f1')\n",
    "    print(f\"\\n{name} Cross-Validation F1-Scores (on training data): {cv_scores}\")\n",
    "    print(f\"Average CV F1-Score for {name}: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6ef08c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping 159 near-constant features: Index(['Feature_1910', 'Feature_1911', 'Feature_1912', 'Feature_1913',\n",
      "       'Feature_1914', 'Feature_1915', 'Feature_1916', 'Feature_1917',\n",
      "       'Feature_1918', 'Feature_1919',\n",
      "       ...\n",
      "       'Feature_2981', 'Feature_2982', 'Feature_3097', 'Feature_3098',\n",
      "       'Feature_3104', 'Feature_3107', 'Feature_3225', 'Feature_3226',\n",
      "       'Feature_3232', 'Feature_3235'],\n",
      "      dtype='object', length=159)\n",
      "Top 10 Feature Importances:\n",
      "Feature_3038    0.005962\n",
      "Feature_1982    0.004320\n",
      "Feature_1905    0.003341\n",
      "Feature_3085    0.002977\n",
      "Feature_3166    0.002975\n",
      "Feature_2965    0.002781\n",
      "Feature_3150    0.002723\n",
      "Feature_1701    0.002636\n",
      "Feature_2909    0.002607\n",
      "Feature_2912    0.002572\n",
      "dtype: float64\n",
      "Predictions for Unseen Dataset:\n",
      "        ID  Predicted_CLASS\n",
      "0   ID_101                0\n",
      "1   ID_102                1\n",
      "2   ID_103                0\n",
      "3   ID_104                0\n",
      "4   ID_105                0\n",
      "5   ID_106                1\n",
      "6   ID_107                0\n",
      "7   ID_108                1\n",
      "8   ID_109                0\n",
      "9   ID_110                0\n",
      "10  ID_111                0\n",
      "11  ID_112                1\n",
      "12  ID_113                0\n",
      "13  ID_114                0\n",
      "14  ID_115                0\n",
      "15  ID_116                0\n",
      "16  ID_117                0\n",
      "17  ID_118                1\n",
      "18  ID_119                1\n",
      "19  ID_120                0\n",
      "20  ID_121                0\n",
      "21  ID_122                0\n",
      "22  ID_123                0\n",
      "23  ID_124                0\n",
      "24  ID_125                0\n",
      "25  ID_126                1\n",
      "26  ID_127                1\n",
      "27  ID_128                0\n",
      "28  ID_129                1\n",
      "29  ID_130                1\n",
      "30  ID_131                0\n",
      "31  ID_132                1\n",
      "32  ID_133                1\n",
      "33  ID_134                1\n",
      "34  ID_135                0\n",
      "35  ID_136                0\n",
      "Predictions saved to 'unseen_predictions.csv'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load the training dataset (for preprocessing consistency)\n",
    "train_df = pd.read_csv(\"../artifacts/train_set.csv\")\n",
    "test_df = pd.read_csv(\"../artifacts/test_set.csv\")\n",
    "\n",
    "# Load the unseen dataset (without CLASS column)\n",
    "unseen_df = pd.read_csv(\"../artifacts/blinded_test_set.csv\")\n",
    "\n",
    "\n",
    "# Step 2: Handle Missing Values (for all datasets)\n",
    "# Drop features 1712 to 1734\n",
    "features_to_drop = [f'Feature_{i}' for i in range(1712, 1735)]\n",
    "train_df = train_df.drop(columns=features_to_drop)\n",
    "unseen_df = unseen_df.drop(columns=[col for col in features_to_drop if col in unseen_df.columns])\n",
    "\n",
    "# Step 3: Reduce Multicollinearity (for all datasets)\n",
    "# Drop Feature_2, Feature_6, Feature_7, Feature_8, Feature_9, Feature_2032\n",
    "cols_to_drop = ['Feature_2', 'Feature_6', 'Feature_7', 'Feature_8', 'Feature_9', 'Feature_2032']\n",
    "train_df = train_df.drop(columns=cols_to_drop)\n",
    "unseen_df = unseen_df.drop(columns=[col for col in cols_to_drop if col in unseen_df.columns])\n",
    "\n",
    "# Step 4: Clip Extreme Values Across All Features (5th to 95th percentiles)\n",
    "numeric_cols = train_df.drop(columns=['ID', 'CLASS']).columns\n",
    "for col in numeric_cols:\n",
    "    # Compute bounds on training data\n",
    "    lower_bound, upper_bound = train_df[col].quantile([0.05, 0.95])\n",
    "    # Apply clipping to both datasets\n",
    "    train_df[col] = train_df[col].clip(lower=lower_bound, upper=upper_bound)\n",
    "    if col in unseen_df.columns:\n",
    "        unseen_df[col] = unseen_df[col].clip(lower=lower_bound, upper=upper_bound)\n",
    "\n",
    "# Step 6: Prepare Features (Skipping Log Transformation)\n",
    "X_train = train_df.drop(columns=['ID', 'CLASS'])\n",
    "# Unseen data has no CLASS column\n",
    "X_unseen = unseen_df.drop(columns=['ID'], errors='ignore')  # Drop ID if present, ignore if not\n",
    "\n",
    "# Step 7: Drop Near-Constant Features (based on training data)\n",
    "stds = X_train.std()\n",
    "low_variance_cols = stds[stds < 1e-6].index\n",
    "print(f\"Dropping {len(low_variance_cols)} near-constant features: {low_variance_cols}\")\n",
    "X_train = X_train.drop(columns=low_variance_cols)\n",
    "X_unseen = X_unseen.drop(columns=[col for col in low_variance_cols if col in X_unseen.columns])\n",
    "\n",
    "# Step 8: Feature Scaling (fit on training data, transform unseen)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "X_unseen_scaled = scaler.transform(X_unseen)\n",
    "X_unseen_scaled = pd.DataFrame(X_unseen_scaled, columns=X_unseen.columns)\n",
    "\n",
    "# Check for infinite/NaN values after scaling\n",
    "for X, name in [(X_train_scaled, 'train'), (X_unseen_scaled, 'unseen')]:\n",
    "    if np.any(np.isinf(X)) or np.any(np.isnan(X)):\n",
    "        print(f\"Infinite or NaN values found in {name} set after scaling. Replacing with 0...\")\n",
    "        X = X.replace([np.inf, -np.inf], 0)\n",
    "        X = X.fillna(0)\n",
    "\n",
    "# Step 9: Feature Selection using Random Forest Importance (based on training data)\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Select top 150 features\n",
    "feature_importance = pd.Series(rf.feature_importances_, index=X_train_scaled.columns)\n",
    "top_features = feature_importance.nlargest(150).index\n",
    "X_train_selected = X_train_scaled[top_features]\n",
    "X_unseen_selected = X_unseen_scaled[top_features]\n",
    "print(\"Top 10 Feature Importances:\")\n",
    "print(feature_importance.nlargest(10))\n",
    "\n",
    "# Step 10: Load the Saved Best Model\n",
    "# Adjust the file name based on the best model saved (e.g., 'logistic_regression_model.pkl')\n",
    "best_model = joblib.load('logistic_regression_model.pkl')  # Update with actual file name\n",
    "\n",
    "# Step 11: Predict on Unseen Dataset\n",
    "y_pred_unseen = best_model.predict(X_unseen_selected)\n",
    "\n",
    "# Create a DataFrame with predictions and ID (if present in unseen data)\n",
    "if 'ID' in unseen_df.columns:\n",
    "    predictions_df = pd.DataFrame({\n",
    "        'ID': unseen_df['ID'],\n",
    "        'Predicted_CLASS': y_pred_unseen\n",
    "    })\n",
    "else:\n",
    "    predictions_df = pd.DataFrame({\n",
    "        'Index': range(len(y_pred_unseen)),\n",
    "        'Predicted_CLASS': y_pred_unseen\n",
    "    })\n",
    "\n",
    "# Output the predictions\n",
    "print(\"Predictions for Unseen Dataset:\")\n",
    "print(predictions_df)\n",
    "\n",
    "# Save predictions to CSV\n",
    "predictions_df.to_csv('unseen_predictions.csv', index=False)\n",
    "print(\"Predictions saved to 'unseen_predictions.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd20a9d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5caf27d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
